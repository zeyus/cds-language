# Session 6 - Word embeddings

## Overview

We were looking in the lecture this week at word embeddings, what they are and how we can use them. We saw how words can be represented as vectors of fixed dimensionality. The numbers in these vectors are learned by a model based on word co-occurrence distributions. Similar vectors appear closer together in space, which means that similar words cluster together more than dissimilar words. 

Word embeddings are hence a way of trying to encode somethign about the *meaning* of individual words and how they relate to each other. If we are going to be using NLP to work with cultural data, being able to account for meaning is vital.

In class today, we'll be seeing a framework for working with word embeddings - both training them and exploring them.

## Tasks

- ```gensim``` walkthrough
- Some exercises with word embeddings
- More on Python scripting
