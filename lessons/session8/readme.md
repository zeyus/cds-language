# Session 8 - Language modelling 2 (Text generation)

## Overview

Last week we saw that RNNs can be really useful for working with language data. This is because the way that RNNs are designed means that they are able to learn *sequential* information from data. This makes them ideal for dealing with natural language data.

Last week, we saw that this means we can use RNNs - especially Bi-LSTMs - to build pretty classifiers which are a bit more sophisticated than using a BoW or Tf-idf model. We also saw that RNNs can be used to perform *language modelling*. This week, we'll be focusing on this second task and thinking about how we can use RNNs for *text generation*.

The in-class work today forms the basis of [Assignment 3](https://classroom.github.com/a/5f7lMH9Y).

## Tasks
- Talking through the notebook
- Some extra python things
  - Fancy requirements files